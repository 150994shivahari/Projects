{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - EEG Emotion Detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import seaborn as sns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./emotions.csv\") # loading the EEG data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n",
      "None\n",
      "shape: (2132, 2549)\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(\"shape:\", df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a     0\n",
       "mean_1_a       0\n",
       "mean_2_a       0\n",
       "mean_3_a       0\n",
       "mean_4_a       0\n",
       "mean_d_0_a     0\n",
       "mean_d_1_a     0\n",
       "mean_d_2_a     0\n",
       "mean_d_3_a     0\n",
       "mean_d_4_a     0\n",
       "mean_d_0_a2    0\n",
       "mean_d_1_a2    0\n",
       "mean_d_2_a2    0\n",
       "mean_d_3_a2    0\n",
       "mean_d_4_a2    0\n",
       "mean_d_5_a     0\n",
       "mean_d_6_a     0\n",
       "mean_d_7_a     0\n",
       "mean_d_8_a     0\n",
       "mean_d_9_a     0\n",
       "mean_d_10_a    0\n",
       "mean_d_11_a    0\n",
       "mean_d_12_a    0\n",
       "mean_d_13_a    0\n",
       "mean_d_14_a    0\n",
       "mean_d_15_a    0\n",
       "mean_d_16_a    0\n",
       "mean_d_17_a    0\n",
       "mean_d_18_a    0\n",
       "mean_d_19_a    0\n",
       "              ..\n",
       "fft_721_b      0\n",
       "fft_722_b      0\n",
       "fft_723_b      0\n",
       "fft_724_b      0\n",
       "fft_725_b      0\n",
       "fft_726_b      0\n",
       "fft_727_b      0\n",
       "fft_728_b      0\n",
       "fft_729_b      0\n",
       "fft_730_b      0\n",
       "fft_731_b      0\n",
       "fft_732_b      0\n",
       "fft_733_b      0\n",
       "fft_734_b      0\n",
       "fft_735_b      0\n",
       "fft_736_b      0\n",
       "fft_737_b      0\n",
       "fft_738_b      0\n",
       "fft_739_b      0\n",
       "fft_740_b      0\n",
       "fft_741_b      0\n",
       "fft_742_b      0\n",
       "fft_743_b      0\n",
       "fft_744_b      0\n",
       "fft_745_b      0\n",
       "fft_746_b      0\n",
       "fft_747_b      0\n",
       "fft_748_b      0\n",
       "fft_749_b      0\n",
       "label          0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # There is no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2676d0115c0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns.countplot(x = \"label\", data = df ) # This shows the count of each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_740_b</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>74.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-534.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2548 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_740_b  fft_741_b  fft_742_b  \\\n",
       "0      -15.70        2.06        3.15  ...       74.3       23.5       20.3   \n",
       "1        2.88        3.83       -4.82  ...      130.0      -23.3      -21.8   \n",
       "2       90.20       89.90        2.03  ...     -534.0      462.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...     -183.0      299.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...      114.0       12.0       38.1   \n",
       "\n",
       "   fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  \n",
       "0       20.3       23.5     -215.0     280.00    -162.00    -162.00     280.00  \n",
       "1      -21.8      -23.3      182.0       2.57     -31.60     -31.60       2.57  \n",
       "2     -233.0      462.0     -267.0     281.00    -148.00    -148.00     281.00  \n",
       "3     -243.0      299.0      132.0     -12.40       9.53       9.53     -12.40  \n",
       "4       38.1       12.0      119.0     -17.60      23.90      23.90     -17.60  \n",
       "\n",
       "[5 rows x 2548 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.drop([\"label\"]  ,axis=1)\n",
    "x.shape\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} ) # converting the categorical to numerical\n",
    "df = df.replace(encode)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.loc[:, \"label\"].values\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # performing Normalization using StandardScalar\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69608594,  0.35491199, -1.21724379, ..., -1.06084278,\n",
       "        -1.06084278,  0.91015287],\n",
       "       [ 0.88626748,  0.65719091,  0.66420935, ...,  0.16401498,\n",
       "         0.16401498, -0.45550631],\n",
       "       [-0.41600022,  0.25775091, -1.50819016, ..., -0.92933964,\n",
       "        -0.92933964,  0.91507541],\n",
       "       ...,\n",
       "       [-1.03421746,  0.13899848, -0.74688051, ...,  0.36220899,\n",
       "         0.36220899, -1.30006583],\n",
       "       [ 0.10098042, -0.76783828, -0.88750458, ..., -2.0846886 ,\n",
       "        -2.0846886 ,  2.24908269],\n",
       "       [ 0.76847442,  0.53843848,  0.66323953, ...,  0.67499859,\n",
       "         0.67499859, -0.50118745]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76847442  0.40889037  0.63317507 ...  0.33121183  0.33121183\n",
      "  -0.4687627 ]\n",
      " [-1.39106498 -3.02845278  0.58710856 ...  0.35751245  0.35751245\n",
      "  -0.42272222]\n",
      " [ 1.13494171  0.75435199  0.63559962 ...  0.46220773  0.46220773\n",
      "  -0.46366296]\n",
      " ...\n",
      " [ 0.22531754  0.20377253 -0.03406192 ... -0.01445355 -0.01445355\n",
      "  -0.16296   ]\n",
      " [-0.10842946 -0.47635504 -0.24742259 ... -0.01539286 -0.01539286\n",
      "  -0.67145797]\n",
      " [-0.96635557 -1.80422316  0.62881087 ...  0.3509373   0.3509373\n",
      "  -0.33426425]]\n",
      "[[-0.2916631   0.40889037  0.69039452 ...  0.55946369  0.55946369\n",
      "  -0.5449488 ]\n",
      " [ 0.63759324  0.4412774   0.69039452 ...  0.47455024  0.47455024\n",
      "  -0.43547159]\n",
      " [ 0.87972342  0.60321253  0.66275462 ...  0.24103825  0.24103825\n",
      "  -0.53018119]\n",
      " ...\n",
      " [ 0.02245172  0.5276428   0.61232391 ...  0.34436214  0.34436214\n",
      "  -0.34804735]\n",
      " [-1.10639842 -2.06871721  0.61280882 ...  0.37535931  0.37535931\n",
      "  -0.30374453]\n",
      " [-0.20004628  0.40889037 -1.81368384 ... -0.95751889 -0.95751889\n",
      "   0.91015287]]\n",
      "[0 1 0 ... 2 2 1]\n",
      "[0 0 0 1 0 0 1 2 2 0 0 2 2 2 2 2 1 1 0 0 1 2 1 2 2 1 2 0 1 0 2 2 0 1 0 0 1\n",
      " 1 0 2 0 2 0 2 1 2 2 1 0 2 1 2 1 0 2 1 1 1 0 0 0 1 0 0 1 2 1 0 0 0 2 1 2 2\n",
      " 1 1 1 1 2 0 0 1 2 0 2 1 1 0 2 2 1 1 2 0 0 1 0 1 2 0 2 2 1 1 2 2 2 0 1 1 1\n",
      " 1 0 2 2 1 1 1 2 1 0 2 0 1 2 0 0 1 0 1 2 1 2 0 0 1 2 0 2 2 2 0 0 1 2 1 1 2\n",
      " 0 1 1 1 0 2 0 1 0 0 1 2 0 0 1 2 0 2 0 2 2 0 1 2 2 0 2 0 0 2 0 0 2 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 0 0 0 0 0 1 2 2 2 2 2 2 1 2 1 2 1 2 2 0 2 2 1 1 0 1 1 2 0\n",
      " 2 2 1 2 0 0 0 0 2 2 2 0 0 0 1 2 2 1 1 1 2 0 0 0 2 0 2 1 2 1 1 0 2 2 0 0 0\n",
      " 2 2 0 2 1 2 2 1 0 2 1 2 1 1 1 2 1 1 2 0 2 1 0 0 2 1 2 2 1 2 0 1 1 1 0 0 2\n",
      " 2 2 1 2 0 2 0 1 1 2 0 2 1 1 2 1 2 0 2 2 0 0 1 1 2 1 0 1 2 2 0 2 1 1 0 1 0\n",
      " 2 1 1 2 0 0 0 0 2 1 0 1 0 1 0 0 1 0 0 2 2 2 1 2 0 1 0 2 1 0 2 1 1 0 0 1 0\n",
      " 0 2 2 0 2 0 2 2 1 1 1 1 2 2 2 2 1 2 1 2 0 2 0 2 0 2 2 0 2 2 0 1 1 2 1 0 0\n",
      " 1 1 2 2 0 0 0 2 1 2 2 2 1 1 0 2 2 1 0 1 2 2 1 1 0 0 0 1 1 2 2 1 1 2 0 2 0\n",
      " 1 0 0 1 2 1 1 2 2 0 1 1 2 0 0 0 1 2 0 1 0 2 1 1 2 2 2 2 2 0 0 1 2 2 2 0 1\n",
      " 1 1 1 0 2 0 0 2 0 2 1 2 0 0 0 2 0 1 0 1 2 1 0 0 1 2 1 2 0 0 1 1 1 2 1 0 0\n",
      " 2 2 0 2 1 0 1 0 0 0 2 2 0 0 1 2 0 0 0 1 1 2 2 2 2 1 2 2 2 0 1 1 1 0 1 0 2\n",
      " 2 2 2 0 2 2 2 1 2 0 0 2 1 1 0 2 0 2 0 2 2 2 0 1 1 0 0 0 1 0 2 0 1 0 2 0 1\n",
      " 2 2 2 0 1 1 2 1 1 1 2 2 2 2 2 0 1 2 2 0 2 0 2 2 2 0 2 0 0 1 0 1 2 1 1 0 0\n",
      " 2 2 1 0 2 0 2 0 1 2 2 1 0 1 2 0 0 2 0 2 2 0 2 2 0 1 2 0 1 2 0 1 1 1 2 1 1\n",
      " 0 1 1 0 2 2 2 0 1 1 1 0 0 0 0 2 1 2 2 2 1 0 0 1 2 2 0 1 1 1 2 0 2 1 0 0 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.33, random_state= 44)\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1])) # reshaping the input to fed into the LSTM network\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             668928    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape = (1,2548), activation = \"relu\", return_sequences = \"True\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(32, activation = \"sigmoid\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation = \"sigmoid\"))\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1428 samples, validate on 704 samples\n",
      "Epoch 1/100\n",
      "1428/1428 [==============================] - 1s 517us/sample - loss: 0.3408 - acc: 0.9272 - val_loss: 0.2833 - val_acc: 0.9375\n",
      "Epoch 2/100\n",
      "1428/1428 [==============================] - 1s 688us/sample - loss: 0.3017 - acc: 0.9321 - val_loss: 0.2612 - val_acc: 0.9375\n",
      "Epoch 3/100\n",
      "1428/1428 [==============================] - 1s 765us/sample - loss: 0.2899 - acc: 0.9279 - val_loss: 0.2297 - val_acc: 0.9432\n",
      "Epoch 4/100\n",
      "1428/1428 [==============================] - 1s 529us/sample - loss: 0.2695 - acc: 0.9314 - val_loss: 0.2247 - val_acc: 0.9361\n",
      "Epoch 5/100\n",
      "1428/1428 [==============================] - 1s 548us/sample - loss: 0.2315 - acc: 0.9433 - val_loss: 0.2068 - val_acc: 0.9474\n",
      "Epoch 6/100\n",
      "1428/1428 [==============================] - 1s 753us/sample - loss: 0.2232 - acc: 0.9454 - val_loss: 0.2052 - val_acc: 0.9375\n",
      "Epoch 7/100\n",
      "1428/1428 [==============================] - 1s 755us/sample - loss: 0.2193 - acc: 0.9440 - val_loss: 0.1842 - val_acc: 0.9474\n",
      "Epoch 8/100\n",
      "1428/1428 [==============================] - 1s 627us/sample - loss: 0.1948 - acc: 0.9503 - val_loss: 0.1896 - val_acc: 0.9389\n",
      "Epoch 9/100\n",
      "1428/1428 [==============================] - 1s 736us/sample - loss: 0.1833 - acc: 0.9482 - val_loss: 0.2068 - val_acc: 0.9361\n",
      "Epoch 10/100\n",
      "1428/1428 [==============================] - 1s 749us/sample - loss: 0.1746 - acc: 0.9573 - val_loss: 0.1527 - val_acc: 0.9545\n",
      "Epoch 11/100\n",
      "1428/1428 [==============================] - 1s 561us/sample - loss: 0.1672 - acc: 0.9580 - val_loss: 0.1738 - val_acc: 0.9375\n",
      "Epoch 12/100\n",
      "1428/1428 [==============================] - 1s 499us/sample - loss: 0.1352 - acc: 0.9608 - val_loss: 0.1546 - val_acc: 0.9531\n",
      "Epoch 13/100\n",
      "1428/1428 [==============================] - 1s 515us/sample - loss: 0.1420 - acc: 0.9622 - val_loss: 0.1492 - val_acc: 0.9517\n",
      "Epoch 14/100\n",
      "1428/1428 [==============================] - 1s 589us/sample - loss: 0.1349 - acc: 0.9629 - val_loss: 0.1350 - val_acc: 0.9602\n",
      "Epoch 15/100\n",
      "1428/1428 [==============================] - 1s 629us/sample - loss: 0.1140 - acc: 0.9720 - val_loss: 0.1311 - val_acc: 0.9574\n",
      "Epoch 16/100\n",
      "1428/1428 [==============================] - 1s 630us/sample - loss: 0.1033 - acc: 0.9699 - val_loss: 0.1351 - val_acc: 0.9588\n",
      "Epoch 17/100\n",
      "1428/1428 [==============================] - 1s 762us/sample - loss: 0.0876 - acc: 0.9832 - val_loss: 0.1334 - val_acc: 0.9616\n",
      "Epoch 18/100\n",
      "1428/1428 [==============================] - 1s 557us/sample - loss: 0.0840 - acc: 0.9832 - val_loss: 0.1201 - val_acc: 0.9659\n",
      "Epoch 19/100\n",
      "1428/1428 [==============================] - 1s 756us/sample - loss: 0.0932 - acc: 0.9790 - val_loss: 0.1313 - val_acc: 0.9602\n",
      "Epoch 20/100\n",
      "1428/1428 [==============================] - 1s 634us/sample - loss: 0.0772 - acc: 0.9839 - val_loss: 0.1613 - val_acc: 0.9460\n",
      "Epoch 21/100\n",
      "1428/1428 [==============================] - 1s 617us/sample - loss: 0.1196 - acc: 0.9678 - val_loss: 0.1455 - val_acc: 0.9616\n",
      "Epoch 22/100\n",
      "1428/1428 [==============================] - 1s 489us/sample - loss: 0.0846 - acc: 0.9811 - val_loss: 0.1242 - val_acc: 0.9730\n",
      "Epoch 23/100\n",
      "1428/1428 [==============================] - 1s 755us/sample - loss: 0.0807 - acc: 0.9825 - val_loss: 0.1577 - val_acc: 0.9602\n",
      "Epoch 24/100\n",
      "1428/1428 [==============================] - 1s 574us/sample - loss: 0.0780 - acc: 0.9790 - val_loss: 0.1679 - val_acc: 0.9503\n",
      "Epoch 25/100\n",
      "1428/1428 [==============================] - 1s 502us/sample - loss: 0.0764 - acc: 0.9839 - val_loss: 0.1535 - val_acc: 0.9645\n",
      "Epoch 26/100\n",
      "1428/1428 [==============================] - 1s 462us/sample - loss: 0.0981 - acc: 0.9741 - val_loss: 0.1273 - val_acc: 0.9716\n",
      "Epoch 27/100\n",
      "1428/1428 [==============================] - 1s 482us/sample - loss: 0.0700 - acc: 0.9832 - val_loss: 0.1190 - val_acc: 0.9702\n",
      "Epoch 28/100\n",
      "1428/1428 [==============================] - 1s 624us/sample - loss: 0.0695 - acc: 0.9818 - val_loss: 0.1488 - val_acc: 0.9645\n",
      "Epoch 29/100\n",
      "1428/1428 [==============================] - 1s 725us/sample - loss: 0.0688 - acc: 0.9839 - val_loss: 0.1450 - val_acc: 0.9688\n",
      "Epoch 30/100\n",
      "1428/1428 [==============================] - 1s 752us/sample - loss: 0.0690 - acc: 0.9797 - val_loss: 0.1277 - val_acc: 0.9673\n",
      "Epoch 31/100\n",
      "1428/1428 [==============================] - 1s 545us/sample - loss: 0.0659 - acc: 0.9790 - val_loss: 0.2213 - val_acc: 0.9503\n",
      "Epoch 32/100\n",
      "1428/1428 [==============================] - 1s 481us/sample - loss: 0.0804 - acc: 0.9762 - val_loss: 0.1429 - val_acc: 0.9659\n",
      "Epoch 33/100\n",
      "1428/1428 [==============================] - 1s 490us/sample - loss: 0.0521 - acc: 0.9888 - val_loss: 0.1162 - val_acc: 0.9744\n",
      "Epoch 34/100\n",
      "1428/1428 [==============================] - 1s 501us/sample - loss: 0.0610 - acc: 0.9818 - val_loss: 0.1362 - val_acc: 0.9702\n",
      "Epoch 35/100\n",
      "1428/1428 [==============================] - 1s 733us/sample - loss: 0.0372 - acc: 0.9930 - val_loss: 0.1371 - val_acc: 0.9702\n",
      "Epoch 36/100\n",
      "1428/1428 [==============================] - 1s 731us/sample - loss: 0.0330 - acc: 0.9944 - val_loss: 0.1261 - val_acc: 0.9730\n",
      "Epoch 37/100\n",
      "1428/1428 [==============================] - 1s 628us/sample - loss: 0.0352 - acc: 0.9937 - val_loss: 0.1432 - val_acc: 0.9702\n",
      "Epoch 38/100\n",
      "1428/1428 [==============================] - 1s 506us/sample - loss: 0.0385 - acc: 0.9916 - val_loss: 0.1306 - val_acc: 0.9716\n",
      "Epoch 39/100\n",
      "1428/1428 [==============================] - 1s 489us/sample - loss: 0.0393 - acc: 0.9902 - val_loss: 0.1388 - val_acc: 0.9673\n",
      "Epoch 40/100\n",
      "1428/1428 [==============================] - 1s 742us/sample - loss: 0.0466 - acc: 0.9874 - val_loss: 0.1330 - val_acc: 0.9716\n",
      "Epoch 41/100\n",
      "1428/1428 [==============================] - 1s 733us/sample - loss: 0.0447 - acc: 0.9895 - val_loss: 0.1318 - val_acc: 0.9744\n",
      "Epoch 42/100\n",
      "1428/1428 [==============================] - 1s 731us/sample - loss: 0.0417 - acc: 0.9923 - val_loss: 0.1267 - val_acc: 0.9744\n",
      "Epoch 43/100\n",
      "1428/1428 [==============================] - 1s 682us/sample - loss: 0.0411 - acc: 0.9902 - val_loss: 0.1438 - val_acc: 0.9688\n",
      "Epoch 44/100\n",
      "1428/1428 [==============================] - 1s 546us/sample - loss: 0.0375 - acc: 0.9923 - val_loss: 0.1889 - val_acc: 0.9574\n",
      "Epoch 45/100\n",
      "1428/1428 [==============================] - 1s 476us/sample - loss: 0.0466 - acc: 0.9888 - val_loss: 0.1595 - val_acc: 0.9659\n",
      "Epoch 46/100\n",
      "1428/1428 [==============================] - 1s 636us/sample - loss: 0.0405 - acc: 0.9895 - val_loss: 0.1633 - val_acc: 0.9659\n",
      "Epoch 47/100\n",
      "1428/1428 [==============================] - 1s 525us/sample - loss: 0.0436 - acc: 0.9888 - val_loss: 0.1560 - val_acc: 0.9688\n",
      "Epoch 48/100\n",
      "1428/1428 [==============================] - 1s 747us/sample - loss: 0.0458 - acc: 0.9902 - val_loss: 0.1722 - val_acc: 0.9673\n",
      "Epoch 49/100\n",
      "1428/1428 [==============================] - 1s 736us/sample - loss: 0.0300 - acc: 0.9930 - val_loss: 0.1749 - val_acc: 0.9673\n",
      "Epoch 50/100\n",
      "1428/1428 [==============================] - 1s 633us/sample - loss: 0.0362 - acc: 0.9923 - val_loss: 0.1577 - val_acc: 0.9659\n",
      "Epoch 51/100\n",
      "1428/1428 [==============================] - 1s 535us/sample - loss: 0.0257 - acc: 0.9958 - val_loss: 0.1736 - val_acc: 0.9688\n",
      "Epoch 52/100\n",
      "1428/1428 [==============================] - 1s 720us/sample - loss: 0.0244 - acc: 0.9937 - val_loss: 0.1911 - val_acc: 0.9659\n",
      "Epoch 53/100\n",
      "1428/1428 [==============================] - 1s 646us/sample - loss: 0.0288 - acc: 0.9951 - val_loss: 0.1850 - val_acc: 0.9659\n",
      "Epoch 54/100\n",
      "1428/1428 [==============================] - 1s 468us/sample - loss: 0.0284 - acc: 0.9937 - val_loss: 0.1833 - val_acc: 0.9673\n",
      "Epoch 55/100\n",
      "1428/1428 [==============================] - 1s 615us/sample - loss: 0.0252 - acc: 0.9965 - val_loss: 0.1809 - val_acc: 0.9673\n",
      "Epoch 56/100\n",
      "1428/1428 [==============================] - 1s 548us/sample - loss: 0.0301 - acc: 0.9930 - val_loss: 0.1538 - val_acc: 0.9716\n",
      "Epoch 57/100\n",
      "1428/1428 [==============================] - 1s 504us/sample - loss: 0.0236 - acc: 0.9965 - val_loss: 0.1797 - val_acc: 0.9645\n",
      "Epoch 58/100\n",
      "1428/1428 [==============================] - 1s 508us/sample - loss: 0.0314 - acc: 0.9923 - val_loss: 0.2157 - val_acc: 0.9616\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428/1428 [==============================] - 1s 747us/sample - loss: 0.0246 - acc: 0.9951 - val_loss: 0.1674 - val_acc: 0.9688\n",
      "Epoch 60/100\n",
      "1428/1428 [==============================] - 1s 755us/sample - loss: 0.0254 - acc: 0.9958 - val_loss: 0.2105 - val_acc: 0.9645\n",
      "Epoch 61/100\n",
      "1428/1428 [==============================] - 1s 757us/sample - loss: 0.0282 - acc: 0.9930 - val_loss: 0.1549 - val_acc: 0.9730\n",
      "Epoch 62/100\n",
      "1428/1428 [==============================] - 1s 731us/sample - loss: 0.0231 - acc: 0.9958 - val_loss: 0.1408 - val_acc: 0.9730\n",
      "Epoch 63/100\n",
      "1428/1428 [==============================] - 1s 677us/sample - loss: 0.0210 - acc: 0.9979 - val_loss: 0.1610 - val_acc: 0.9688\n",
      "Epoch 64/100\n",
      "1428/1428 [==============================] - 1s 627us/sample - loss: 0.0138 - acc: 0.9986 - val_loss: 0.1614 - val_acc: 0.9688\n",
      "Epoch 65/100\n",
      "1428/1428 [==============================] - 1s 759us/sample - loss: 0.0210 - acc: 0.9979 - val_loss: 0.1915 - val_acc: 0.9688\n",
      "Epoch 66/100\n",
      "1428/1428 [==============================] - 1s 672us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.1804 - val_acc: 0.9688\n",
      "Epoch 67/100\n",
      "1428/1428 [==============================] - 1s 751us/sample - loss: 0.0218 - acc: 0.9951 - val_loss: 0.1756 - val_acc: 0.9702\n",
      "Epoch 68/100\n",
      "1428/1428 [==============================] - 1s 755us/sample - loss: 0.0228 - acc: 0.9937 - val_loss: 0.1893 - val_acc: 0.9659\n",
      "Epoch 69/100\n",
      "1428/1428 [==============================] - 1s 699us/sample - loss: 0.0295 - acc: 0.9923 - val_loss: 0.1929 - val_acc: 0.9645\n",
      "Epoch 70/100\n",
      "1428/1428 [==============================] - 1s 595us/sample - loss: 0.0232 - acc: 0.9965 - val_loss: 0.1681 - val_acc: 0.9688\n",
      "Epoch 71/100\n",
      "1428/1428 [==============================] - 1s 749us/sample - loss: 0.0306 - acc: 0.9923 - val_loss: 0.1571 - val_acc: 0.9759\n",
      "Epoch 72/100\n",
      "1428/1428 [==============================] - 1s 745us/sample - loss: 0.0249 - acc: 0.9951 - val_loss: 0.1473 - val_acc: 0.9773\n",
      "Epoch 73/100\n",
      "1428/1428 [==============================] - 1s 737us/sample - loss: 0.0386 - acc: 0.9888 - val_loss: 0.1471 - val_acc: 0.9787\n",
      "Epoch 74/100\n",
      "1428/1428 [==============================] - 1s 525us/sample - loss: 0.0344 - acc: 0.9909 - val_loss: 0.1657 - val_acc: 0.9702\n",
      "Epoch 75/100\n",
      "1428/1428 [==============================] - 1s 557us/sample - loss: 0.0366 - acc: 0.9895 - val_loss: 0.1518 - val_acc: 0.9688\n",
      "Epoch 76/100\n",
      "1428/1428 [==============================] - 1s 523us/sample - loss: 0.0536 - acc: 0.9860 - val_loss: 0.1222 - val_acc: 0.9744\n",
      "Epoch 77/100\n",
      "1428/1428 [==============================] - 1s 763us/sample - loss: 0.0351 - acc: 0.9909 - val_loss: 0.1616 - val_acc: 0.9688\n",
      "Epoch 78/100\n",
      "1428/1428 [==============================] - 1s 741us/sample - loss: 0.0345 - acc: 0.9888 - val_loss: 0.1210 - val_acc: 0.9801\n",
      "Epoch 79/100\n",
      "1428/1428 [==============================] - 1s 753us/sample - loss: 0.0334 - acc: 0.9916 - val_loss: 0.1469 - val_acc: 0.9702\n",
      "Epoch 80/100\n",
      "1428/1428 [==============================] - 1s 750us/sample - loss: 0.0420 - acc: 0.9916 - val_loss: 0.1540 - val_acc: 0.9716\n",
      "Epoch 81/100\n",
      "1428/1428 [==============================] - 1s 626us/sample - loss: 0.0313 - acc: 0.9944 - val_loss: 0.1380 - val_acc: 0.9744\n",
      "Epoch 82/100\n",
      "1428/1428 [==============================] - 1s 518us/sample - loss: 0.0304 - acc: 0.9923 - val_loss: 0.1563 - val_acc: 0.9702\n",
      "Epoch 83/100\n",
      "1428/1428 [==============================] - 1s 728us/sample - loss: 0.0351 - acc: 0.9902 - val_loss: 0.1690 - val_acc: 0.9702\n",
      "Epoch 84/100\n",
      "1428/1428 [==============================] - 1s 738us/sample - loss: 0.0303 - acc: 0.9909 - val_loss: 0.1570 - val_acc: 0.9716\n",
      "Epoch 85/100\n",
      "1428/1428 [==============================] - 1s 712us/sample - loss: 0.0314 - acc: 0.9937 - val_loss: 0.1540 - val_acc: 0.9702\n",
      "Epoch 86/100\n",
      "1428/1428 [==============================] - 1s 748us/sample - loss: 0.0239 - acc: 0.9944 - val_loss: 0.1978 - val_acc: 0.9631\n",
      "Epoch 87/100\n",
      "1428/1428 [==============================] - 1s 595us/sample - loss: 0.0307 - acc: 0.9930 - val_loss: 0.1662 - val_acc: 0.9716\n",
      "Epoch 88/100\n",
      "1428/1428 [==============================] - 1s 494us/sample - loss: 0.0358 - acc: 0.9888 - val_loss: 0.1703 - val_acc: 0.9702\n",
      "Epoch 89/100\n",
      "1428/1428 [==============================] - 1s 533us/sample - loss: 0.0345 - acc: 0.9909 - val_loss: 0.1442 - val_acc: 0.9744\n",
      "Epoch 90/100\n",
      "1428/1428 [==============================] - 1s 500us/sample - loss: 0.0314 - acc: 0.9909 - val_loss: 0.1486 - val_acc: 0.9730\n",
      "Epoch 91/100\n",
      "1428/1428 [==============================] - 1s 756us/sample - loss: 0.0319 - acc: 0.9923 - val_loss: 0.1452 - val_acc: 0.9759\n",
      "Epoch 92/100\n",
      "1428/1428 [==============================] - 1s 715us/sample - loss: 0.0314 - acc: 0.9916 - val_loss: 0.1392 - val_acc: 0.9759\n",
      "Epoch 93/100\n",
      "1428/1428 [==============================] - 1s 579us/sample - loss: 0.0295 - acc: 0.9930 - val_loss: 0.1487 - val_acc: 0.9773\n",
      "Epoch 94/100\n",
      "1428/1428 [==============================] - 1s 564us/sample - loss: 0.0227 - acc: 0.9951 - val_loss: 0.1689 - val_acc: 0.9702\n",
      "Epoch 95/100\n",
      "1428/1428 [==============================] - 1s 500us/sample - loss: 0.0205 - acc: 0.9951 - val_loss: 0.1706 - val_acc: 0.9730\n",
      "Epoch 96/100\n",
      "1428/1428 [==============================] - 1s 730us/sample - loss: 0.0262 - acc: 0.9930 - val_loss: 0.1559 - val_acc: 0.9744\n",
      "Epoch 97/100\n",
      "1428/1428 [==============================] - 1s 717us/sample - loss: 0.0229 - acc: 0.9944 - val_loss: 0.1589 - val_acc: 0.9759\n",
      "Epoch 98/100\n",
      "1428/1428 [==============================] - 1s 721us/sample - loss: 0.0132 - acc: 0.9979 - val_loss: 0.1557 - val_acc: 0.9759\n",
      "Epoch 99/100\n",
      "1428/1428 [==============================] - 1s 752us/sample - loss: 0.0293 - acc: 0.9944 - val_loss: 0.2023 - val_acc: 0.9688\n",
      "Epoch 100/100\n",
      "1428/1428 [==============================] - 1s 645us/sample - loss: 0.0380 - acc: 0.9916 - val_loss: 0.1633 - val_acc: 0.9730\n",
      "704/704 [==============================] - 0s 137us/sample - loss: 0.1633 - acc: 0.9730\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data = (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Emotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.2067895e-01 2.1600723e-04 9.8854303e-05]\n",
      " [8.2061028e-01 2.1609664e-04 9.8884106e-05]\n",
      " [8.2071912e-01 2.1600723e-04 9.8764896e-05]\n",
      " ...\n",
      " [8.2071376e-01 2.1600723e-04 9.8824501e-05]\n",
      " [6.8008900e-05 8.0909491e-01 4.3484569e-04]\n",
      " [5.9217215e-05 5.9664249e-04 7.1335816e-01]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.30%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
